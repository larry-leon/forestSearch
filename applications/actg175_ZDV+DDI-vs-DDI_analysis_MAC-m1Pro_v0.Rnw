\documentclass[9pt]{article}
\usepackage{multicol}
\usepackage{amssymb,mathrsfs,graphicx}
\usepackage{enumerate}
\usepackage{amsthm}
\usepackage{lscape}
\usepackage{pdfpages}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem{remark}{Remark}

\usepackage[colorlinks=true,linkcolor={blue},citecolor={blue},urlcolor={blue}]{hyperref}
\usepackage{longtable,ctable}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{mathptmx}
\usepackage{natbib}
\usepackage{setspace}

\usepackage[utf8]{inputenc}
\usepackage{pgf}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumerate}
\usepackage{threeparttable}


\newcommand{\R}{R}
\newcommand{\gbsg}{gbsg}

\def\FsNg{\hbox{FS}(M_{g})}

\def\hhat{\hat\theta(\hat{H})}
\def\hchat{\hat\theta(\hat{H}^{c})}
\def\hknow{\hat\theta(H)}
\def\hcknow{\hat\theta(H^{c})}
\def\hplim{\theta^{\dagger}(H)}
\def\hcplim{\theta^{\dagger}(H^{c})}

\newcommand{\indep}{\perp \!\!\! \perp}

\textheight=9.25in \textwidth=6.0in 
\topmargin=0in
\evensidemargin=0in \oddsidemargin=0in

\begin{document}

\raggedbottom


<<echo=TRUE>>=
opts_chunk$set (warning = FALSE, message = FALSE, tidy=TRUE, echo=TRUE)
options(warn = -1)

rm(list=ls())

library(survival)
library(knitr)
library(kableExtra)

library(glmnet)

library(ggplot2)

# Following loaded in "forest_search_v0.R"
suppressMessages(library(randomForest))
#library(SPlit)

library(grf)
library(policytree)
library(DiagrammeR)

#library(cowplot)

library(data.table)
library(plyr)
library(aVirtualTwins)
# Not sure formatR is needed?
#library(formatR)
suppressMessages(library(gridExtra))

library(speff2trial)

# Location where code is stored
# Modified for MAC
codepath<-c("/Users/larryleon/Documents/GitHub/forestSearch//R/")
source(paste0(codepath,"source_forestsearch_v0.R"))
source_fs_functions(file_loc=codepath)
@


<<echo=TRUE>>=
t.start.all<-proc.time()[3]
# GRF analysis
# To guide selection of binary cutpoints
df.analysis<-subset(ACTG175,arms %in% c(2,3))

df.analysis<-within(df.analysis,{
id<-as.numeric(c(1:nrow(df.analysis)))  
time_days<-days
treat<-ifelse(arms==2,1,0)
})

#plot(survfit(Surv(time_days,cens)~treat,data=df.analysis))
coxph(Surv(time_days,cens)~treat,data=df.analysis)

confounders.name<-c("age","wtkg","karnof","cd40","cd80","hemo","homo","drugs","race","gender","oprior","symptom")
outcome.name<-c("time_days")
event.name<-c("cens")
id.name<-c("id")
treat.name<-c("treat")

n.min<-60
dmin.grf<-12
frac.tau<-0.80

grf.est<-grf.subg.harm.survival(data=df.analysis,confounders.name=confounders.name,outcome.name=outcome.name,
event.name=event.name,id.name=id.name,treat.name=treat.name,n.min=n.min,dmin.grf=dmin.grf,frac.tau=frac.tau,details=TRUE)

cat("Truncation point for RMST:",c(grf.est$tau.rmst),"\n")

# Plot manually

#plot(grf.est$tree)

#plot(grf.est$tree1)

#plot(grf.est$tree2)

#plot(grf.est$tree3)

df0.grf<-subset(grf.est$data,treat.recommend==0)
df1.grf<-subset(grf.est$data,treat.recommend==1)

# Terminal leaf corresponding to selected SG
cat("Terminal leaf:",c(grf.est$sg.harm.id),"\n")
# action=1 --> recommend control

# Manually identify the subgroup looking at tree
# and terminal leaf
print(dim(df0.grf))
check<-subset(df.analysis, karnof<=90 & cd80>1034 & age>37)
print(dim(check))

#plot(survfit(Surv(time_days,cens)~treat,data=df.analysis))
#coxph(Surv(time_days,cens)~treat,data=df.analysis)

save(grf.est,file="output/grf_actg_Arms_2vs3_final.Rdata")
@



<<echo=TRUE>>=
t.done<-proc.time()[3]
t.min<-(t.done-t.start.all)/60
cat("Minutes and hours for GRF estimation",c(t.min,t.min/60),"\n")
@

\begin{figure}[h!]
\begin{center}
<<grf_sg,echo=TRUE,out.height="400px",out.width="400px">>=
par(mfrow=c(1,2))
plot.subgroup(sub1=df0.grf,sub1C=df1.grf,tte.name="time_days",event.name="cens",treat.name="treat",fix.rows=FALSE,byrisk=200,show.med=FALSE,ymin=0.4)
@
\end{center}
\end{figure}


<<echo=TRUE>>=
t.start<-proc.time()[3]

cat("GRF variables in selected tree","\n")
print(grf.est$tree.names)

cat("GRF cuts wrt selected tree:","\n")
print(grf.est$tree.cuts)

# Reduce dimension via Cox lasso

xx<-as.matrix(df.analysis[,confounders.name])
yy<-as.matrix(df.analysis[,c("time_days","cens")])
colnames(yy)<-c("time","status")

cvfit <- cv.glmnet(xx, yy, family="cox") #first do 10-fold cross-validation to select lambda

m <- glmnet(xx, yy, family="cox", lambda=cvfit$lambda.min) #plugin the optimal lambda

conflasso.name<-confounders.name[which(m$beta!=0)]

cat("Cox-LASSO selected:",c(conflasso.name),"\n")

cat("GRF cuts wrt selected tree:","\n")
print(grf.est$tree.cuts)

# Considering continuous factors per GRF cuts
# Only also considering drugs and symptom per lasso
df.analysis<-within(df.analysis,{
z1a<-ifelse(age<=37,1,0)
z1b<-ifelse(age<=median(age),1,0)
z2<-ifelse(wtkg<=65,1,0)
z3<-ifelse(karnof<=90,1,0)
z4<-ifelse(cd40<=417,1,0)
z5a<-ifelse(cd80<=499,1,0)
z5b<-ifelse(cd80<=680,1,0)
z5c<-ifelse(cd80<=1034,1,0)
#z6<-hemo
#z7<-homo
z8<-drugs
#z9<-race
#z10<-gender
#z11<-oprior
z12<-symptom
# Convert to factors
v1a<-as.factor(z1a)
v1b<-as.factor(z1b)
v2<-as.factor(z2)
v3<-as.factor(z3)
v4<-as.factor(z4)
v5a<-as.factor(z5a)
v5b<-as.factor(z5b)
v5c<-as.factor(z5c)
v6<-as.factor(z8)
v7<-as.factor(z12)
})

FSconfounders.name<-c("v1a","v1b",
"v2","v3","v4",
"v5a","v5b","v5c",
"v6","v7")

outcome.name<-c("time_days")
event.name<-c("cens")
id.name<-c("id")
treat.name<-c("treat")

df.confounders<-df.analysis[,FSconfounders.name]
df.confounders<-dummy(df.confounders)

hr.threshold<-1.5   # Initital candidates 
hr.consistency<-1.25 # Candidates for many splits

pconsistency.threshold<-0.90
maxk<-4
# maxk is max # of covariates in combination
# Since we want to allow generation of intervals for single covariate
# allowing for 4 can yield v1, v2 (say), and v3,v4 with v3 and v4 
# generating intervals for a single covariate

# Limit timing for forestsearch
max.minutes<-60.0
nmin.fs<-60
#stop.threshold<-0.60 # If any sg meets this, then choose this (stop here);
m1.threshold<-Inf # Turning this off (Default)
stop.threshold<-1.0 
# =1 will run through all sg's meeting HR criteria
fs.splits<-1000 # How many times to split for consistency
# vi is % factor is selected in cross-validation --> higher more important
vi.grf.min<-0.2
# Null, turns off grf screening
# Set to 5 for this heavily censored data
d.min<-5 # Min number of events for both arms (d0.min=d1.min=d.min)
# default=5

sg_focus<-"hr"
split_method<-"Random"
pstop_futile<-0.3
# Stops the consistency evaluation after first subgroup with consistency below pstop_futile
# With idea that since SG's are sorted by hazard ratio estimates, once consistency is below 
# pstop_futile it seems unlikely that SG's with lower hr's will reach the required 
# consistency criterion


fs.est<-forestsearch(df=df.analysis,confounders.name=FSconfounders.name,df.predict=df.analysis,details=TRUE,
sg_focus=sg_focus,
split_method=split_method,
pstop_futile=pstop_futile,
outcome.name=outcome.name,treat.name=treat.name,event.name=event.name,id.name=id.name,
n.min=nmin.fs,hr.threshold=hr.threshold,hr.consistency=hr.consistency,
fs.splits=fs.splits,
stop.threshold=stop.threshold,d0.min=d.min,d1.min=d.min,
pconsistency.threshold=pconsistency.threshold,max.minutes=max.minutes,maxk=maxk,plot.sg=FALSE,vi.grf.min=vi.grf.min)

xx<-fs.est$find.grps$out.found$hr.subgroups
covs.found<-xx[,-c(1:10)]
covs.most<-apply(covs.found,2,sum)
covs.most<-covs.most[covs.most>0]
print(covs.most)

print(fs.est$grp.consistency$result)

df0.fs<-subset(fs.est$df.pred,treat.recommend==0)
df1.fs<-subset(fs.est$df.pred,treat.recommend==1)

save(fs.est,df.analysis,FSconfounders.name,file="output/fs_actg_Arms_2vs3_final.Rdata")
@


<<echo=TRUE>>=
t.done<-proc.time()[3]
t.min<-(t.done-t.start)/60
cat("Minutes and hours for FS estimation",c(t.min,t.min/60),"\n")
@


\begin{figure}[h!]
\begin{center}
<<grf-fs_sg,echo=TRUE,out.height="400px",out.width="400px">>=
# Compare with GRF
layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
plot.subgroup(sub1=df0.grf,sub1C=df1.grf,tte.name="time_days",event.name="cens",treat.name="treat",fix.rows=FALSE,byrisk=200,show.med=FALSE,ymin=0.4)
plot.subgroup(sub1=df0.fs,sub1C=df1.fs,tte.name="time_days",event.name="cens",treat.name="treat",fix.rows=FALSE,byrisk=200,show.med=FALSE,ymin=0.4)
@
\end{center}
\end{figure}


<<echo=TRUE,eval=TRUE>>=
t.start<-proc.time()[3]
# Note, the elements above will need to be re-initiated 
# if running separate from above
# E.g., outcome.names, event.name, ... hr.threshold, etc.

#load("output/fs_actg_Arms_2vs3_final.Rdata")

library(doParallel)
registerDoParallel(parallel::detectCores(logical = FALSE))

cox.formula.boot<-as.formula(paste("Surv(time_days,cens)~treat"))
split_method<-"Random"
est.loghr<-TRUE

confounders.name<-FSconfounders.name
stop.threshold<-0.99
fs.splits<-1000 
max.minutes<-6.0

# Suggest running 50, first ... to get timing estimate
NB<-2000

df_temp<-fs.est$df.pred[,c("id","treat.recommend")]
dfa<-merge(df.analysis,df_temp,by="id")
df_boot_analysis<-dfa

fitH<-get_Cox_sg(df_sg=subset(df_boot_analysis,treat.recommend==0),cox.formula=cox.formula.boot,est.loghr=est.loghr)
H_obs<-fitH$est_obs # log(hr) scale
seH_obs<-fitH$se_obs
# Hc observed estimates
fitHc<-get_Cox_sg(df_sg=subset(df_boot_analysis,treat.recommend==1),cox.formula=cox.formula.boot,est.loghr=est.loghr)
Hc_obs<-fitHc$est_obs
seHc_obs<-fitHc$se_obs
rm("fitH","fitHc")

Ystar_mat<-bootYstar(
{  
ystar<-get_Ystar(boot)
},
boots=NB,
seed=8316951,
counter="boot",
export=fun_arg_list_boot
)
# Check dimension
if(dim(Ystar_mat)[1]!=NB | dim(Ystar_mat)[2]!=nrow(df_boot_analysis)) stop("Dimension of Ystar_mat does not match")

tB.start<-proc.time()[3]
# Bootstraps
resB<-bootPar(
{  
ans<-fsboot_forparallel(boot)
},
boots=NB,
seed=8316951,
counter="boot",
export=fun_arg_list_boot
)
tB.now<-proc.time()[3]
tB.min<-(tB.now-tB.start)/60

doParallel::stopImplicitCluster()

cat("Minutes for Boots",c(NB,tB.min),"\n")
cat("Projection per 100",c(tB.min*(100/NB)),"\n")
cat("Propn bootstrap subgroups found =",c(sum(!is.na(resB$H_biasadj_1))/NB),"\n")
# How many timmed out 
cat("Number timmed out=",c(sum(is.na(resB$H_biasadj_1) & resB$tmins_search>max.minutes)),"\n")

H_estimates<-get_dfRes(Hobs=H_obs,seHobs=seH_obs,H1_adj=resB$H_biasadj_1,ystar=Ystar_mat,cov_method="standard",cov_trim=0.05)

Hc_estimates<-get_dfRes(Hobs=Hc_obs,seHobs=seHc_obs,H1_adj=resB$Hc_biasadj_1,ystar=Ystar_mat,cov_method="standard",cov_trim=0.05)

print(H_estimates)
print(Hc_estimates)

save(fs.est,Ystar_mat,resB,H_estimates,Hc_estimates,df_boot_analysis,file="output/fsBoot_actg_Arms_2vs3__final.Rdata")

@


<<echo=TRUE>>=
t.done<-proc.time()[3]
t.min<-(t.done-t.start)/60
cat("Minutes and hours for FS bootstrap",c(t.min,t.min/60),"\n")
@


<<echo=TRUE>>=

df0.fs<-subset(fs.est$df.pred,treat.recommend==0)
df1.fs<-subset(fs.est$df.pred,treat.recommend==1)

# ITT analysis
cox_itt<-summary(coxph(Surv(time_days,cens)~treat,data=fs.est$df.pred))$conf.int

  # ITT estimates
resITT<-c(round(cox_itt[c(1,3,4)],2),nrow(fs.est$df.pred))

# Forest Search
# Un-adjusted
Hstat<-c(unlist(H_estimates))[c(1,3,4)]
resH_obs<-c(c(Hstat),nrow(df0.fs))
# Bias-corrected
Hstat<-c(unlist(H_estimates))[c(5,7,8)]
resH_bc<-c(c(Hstat),nrow(df0.fs))

Hstat2<-c(unlist(H_estimates))[c(5,7,8)]
Hstat2<-round(Hstat2,2)
a<-paste0(Hstat2[1]," [")
a<-paste0(a,Hstat2[2])
a<-paste0(a,",")
a<-paste0(a,Hstat2[3])
a<-paste0(a,"]")
H_bc2<-c(a)

# Un-adjusted
Hcstat<-c(unlist(Hc_estimates))[c(1,3,4)]
resHc_obs<-c(c(Hcstat),nrow(df1.fs))
# Bias-corrected
Hcstat<-c(unlist(Hc_estimates))[c(5,7,8)]
resHc_bc<-c(c(Hcstat),nrow(df1.fs))


Hcstat2<-c(unlist(Hc_estimates))[c(5,7,8)]
Hcstat2<-round(Hcstat2,2)
a<-paste0(Hcstat2[1]," [")
a<-paste0(a,Hcstat2[2])
a<-paste0(a,",")
a<-paste0(a,Hcstat2[3])
a<-paste0(a,"]")
Hc_bc2<-c(a)

res<-rbind(resITT,resH_obs,resH_bc,resHc_obs,resHc_bc)

resf<-as.data.frame(res)

colnames(resf)<-c("HR Estimate","Lower","Upper","$\\#$ Subjects")

rnH<-c("$\\hat{H}$","$\\hat{H}_{bc}$")
rnHc<-c("$\\hat{H}^{c}$","$\\hat{H}^{c}_{bc}$")
rnItt<-c("ITT")
rownames(resf)<-c(rnItt,rnH,rnHc)
@


<<fs_tab,echo=TRUE>>=
# Resolve conflict with dplyr 
library(conflicted)
group_rows<-kableExtra::group_rows

options(knitr.kable.NA = '.',format="latex")
tab_actg<-kbl(resf,longtable=FALSE,align='c',format="latex",booktabs=TRUE,escape=F,digits=3,
caption="\\label{tab:actg} ACTG-175 FS Analysis: Cox hazard ratio (HR) estimates for the ITT population and subgroups $H$ and $H^{c}$.
Cox model estimates are based on subgroups: H true (knowing the actual subgroup, a-priori); the estimated subgroup $\\hat{H}$; and 
the bootstrap ($B=2,000$) bias-correction to $\\hat{H}$ estimates, denoted $\\hat{H}_{bc}$.  Estimates for the complement $H^{c}$ are defined analogously.
The number of subjects in each population ($\\#$ Subjects) are listed.") %>%
kable_styling(full_width = FALSE, font_size=9,latex_options="hold_position") %>%
group_rows("ITT", 1, 1) %>%
group_rows("H subgroup estimates", 2, 3) %>%
group_rows("H-complement subgroup estimates", 4, 5)
@

\Sexpr{tab_actg}

<<echo=TRUE>>=
t.done<-proc.time()[3]
t.min<-(t.done-t.start.all)/60
cat("Minutes and hours to finish",c(t.min,t.min/60),"\n")
cat("Machine=",c(Sys.info()[[4]]),"\n")
cat("Number of cores=",c(detectCores(logical = FALSE)),"\n")
@


\end{document}


